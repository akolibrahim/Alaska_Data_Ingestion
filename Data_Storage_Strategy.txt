Data Storage Strategy in a DB

1) Load Adobe Analytics data feed into Azure Data Lake
	A) Adobe data feeds support Azure Blob destinations. After that we can copy data from Azure Blob to Azure SQL Database using ADF (data pipeline). 
		Alternatively, we can create Databricks table from Azure Blob.
	B) Our "Search" and "Search Results" tables would look similar to below structures (high level structure): 
		
		create table dbo.search
		(
			from_ varchar(120) Not NULL,
			to_ varchar(120),
			departure_date date Not NULL,
			return_date date,
			adults varchar(16) Not NULL,
			children varchar(16),
			load_date date,
			visitor_id integer Not NULL Primary Key,
			date_time date
		)
		
		create table dbo.search_results
		(
			Departing_Flight varchar(120) Not NULL,
			Returning_Flight varchar(120),
			Stops varchar(16),
			Airlines varchar(64),
			Nearby_Airports varchar(64),
			Upgrades varchar(64),
			Sortby varchar(32),
			view_type varchar(16),
			load_date date,
			visitor_id integer Not NULL Primary Key,
			date_time date
		)
		
		* Loading the Blob data into Databricks Table (example):
		Clickstream_Dat=spark.read.format("parquet")\
		  .option("inferSchema", "false")\
		  .option("header", "true")\
		  .option("sep", ",")\
		  .load(clickstream_dat_loc)
		  
		clickstream_dat.write.format("parquet").mode("append").saveAsTable('dbo.clickstream')